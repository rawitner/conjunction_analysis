---
output: html_document
title: Conjunction Analysis
---

```{r include=F}
knitr::opts_chunk$set(echo=F)
knitr::opts_chunk$set(warning=F)
knitr::opts_chunk$set(message=F)

setwd("~/Documents/Centauri/fall part time/conjunction_analysis/")
library(tidyverse)
library(readr)
library(kableExtra)
library(knitr)
library(lubridate)
library(DT)
library(RColorBrewer)
library(stringr)
Sys.setenv(TZ='EST')
mcma_objs = readRDS("RDSfiles/mcma_objs")
all_conjs = readRDS("RDSfiles/all_conjs")
all_conjs_expanded = readRDS("RDSfiles/all_conjs_expanded")
derelicts = readRDS("RDSfiles/derelicts")
derelictDat = readRDS("RDSfiles/derelictDatNew")
file_list = readRDS("RDSfiles/file_list")
today = toupper(strftime(Sys.Date(), format="%d%b%Y")) # current day
path = "conj_data/"
```

```{r update_conj_files, eval=F}
# add new conjunction files to all_conjs dataframe

# read in new conjunction files
file_list_new = list.files(path)
file_list_new = file_list_new[!(file_list_new %in% file_list)] # only the new conjunctions

colnames = c("PrimarySatellite","SecondarySatellite","TCA_EpDay",
             "TCA_UTCG","Range","RangeX","RangeY","RangeZ","Velocity",
             "VelocityX","VelocityY","VelocityZ","Latitude","Longitude",
             "Altitude","PrimaryAge","SecondaryAge","PrimaryCluster",
             "SecondaryCluster","DateGenerated","del")

all_conjs_new = data.frame()
for (i in 1:length(file_list_new)) {
  file = paste0(path, file_list_new[i])
  print(file)
  
  firstLine = readLines(file, n=2)[2]
  
  if (str_count(firstLine, ',') == 20) { # if file has trailing commas
    temp_data = read_csv(file, skip=1, col_names = colnames, col_types = "ccncnnnnnnnnnnncccccc") %>%
      select(-del)
  } else {
    temp_data = read_csv(file, skip=1, col_names = colnames[-length(colnames)], col_types = "ccncnnnnnnnnnnnccccc")
  }
  
  all_conjs_new = rbind(all_conjs_new, temp_?padata) #for each iteration, bind the new data to the building dataset
}

mycols <- '(PrimaryCluster, SecondaryCluster)'
minf <- paste0('min',mycols)
maxf <- paste0('max',mycols)

all_conjs_new = all_conjs_new %>%
  mutate(DateGenerated = parse_date_time(DateGenerated, tz="EST", 
                                   orders=c("%Y-%m-%d %H:%M:%S", "%m/%d/%y %H:%M")),
         date = DateGenerated - 24*60*60,
         utcg = if_else(nchar(TCA_UTCG) > 7,
                        as.POSIXct(TCA_UTCG, format="%Y-%m-%d %H:%M:%S"),
                        date + TCA_EpDay*24*60*60),
         TCA_UTCG = utcg) %>% 
  select(-c(date, utcg)) %>%
  rowwise() %>% 
  mutate(firstClust = eval(parse(text=minf)),
         secondClust = eval(parse(text=maxf)),
         clusters = paste(firstClust, secondClust, sep="-")) %>% 
  ungroup() %>%
  mutate(clusterLab = if_else(firstClust=="LEO" & secondClust=="LEO", "LEO",
                              if_else(secondClust=="LEO" & firstClust!="LEO", "LEO-other",
                                      if_else(firstClust=="HIGH" & secondClust=="HIGH", "HIGH",
                                              if_else(firstClust=="HIGH" & secondClust!="HIGH", 
                                                      "HIGH-other", firstClust)))),
         clusterLab = factor(clusterLab,
                             levels = c("615", "775", "850", "975", "1200", "1500", "LEO","LEO-other","HIGH-other"),
                             ordered = T))



# update file list
file_list = append(file_list, file_list_new)
saveRDS(file_list, "RDSfiles/file_list")

#########
# WORST OFFENDER alg for new conjunctions
# persistence
alts = c(775,850,975,1500)
pers = c(300,500,1800,20000)
persistence = cbind(alts, pers) %>% as_tibble()
model = lm(log(pers) ~ alts, persistence)
intercept = as.numeric(model$coefficients[1])
slope = as.numeric(model$coefficients[2])

# get operational satellites
opSats = derelictDat %>% filter(avgAlt < 2000 & operational) # df of operational sats
combinedMass_v = vector()
persistence_v = vector()
numOpSats_v = vector()
for (i in 1:nrow(all_conjs_new)) {
  conj = all_conjs_new[i, ]
  noradId1 = gsub("--.*", "", conj$PrimarySatellite)
  noradId2 = gsub("--.*", "", conj$SecondarySatellite)
  obj1 = filter(mcma_objs, noradId == noradId1)
  obj2 = filter(mcma_objs, noradId == noradId2)
  
  combinedMass = as.numeric(obj1$mass) + as.numeric(obj2$mass)
  persistence = exp(intercept + slope * conj$Altitude)
  
  numOpSats = 0
  for (j in 1:nrow(opSats)) {
    # count how many op sats overlap in altitude
    opSat = opSats[j,]
    conjMinAlt = conj$Altitude - 100
    conjMaxAlt = conj$Altitude + 100
    if ((opSat$perigee > conjMinAlt &
         opSat$apogee < conjMaxAlt) ||
        (opSat$perigee < conjMaxAlt &
         opSat$apogee > conjMaxAlt) ||
        (opSat$perigee < conjMinAlt &
         opSat$apogee > conjMinAlt)) {
      numOpSats = numOpSats + 1
    }
  }
  
  combinedMass_v = append(combinedMass_v, toString(combinedMass))
  persistence_v = append(persistence_v, persistence)
  numOpSats_v = append(numOpSats_v, numOpSats)
}
all_conjs_new$combinedMass = combinedMass_v
all_conjs_new$persistence = persistence_v
all_conjs_new$numOpSats = numOpSats_v

all_conjs_new = all_conjs_new %>%
  mutate(combinedMass = if_else( grepl(",", combinedMass, fixed = T), # if it contains a comma
                                 as.numeric(gsub(",.*", "", combinedMass)), # make substring up to the comma
                                 as.numeric(combinedMass) )) %>% # otherwise don't change
  mutate(risk = (combinedMass * persistence * numOpSats) / Range)

# append new conjunctions to previous
all_conjs = rbind(all_conjs, all_conjs_new)
saveRDS(all_conjs, "RDSfiles/all_conjs") # save to RDS file

# sum risk for each object:
# list all conjunctions by first sat, then by second sat, then bind by rows
firstSet = all_conjs %>%
  mutate(noradId = as.numeric(gsub("--.*", "", PrimarySatellite))) %>%
  dplyr::select(-c(PrimarySatellite, SecondarySatellite))

secondSet = all_conjs %>%
  mutate(noradId = as.numeric(gsub("--.*", "", SecondarySatellite))) %>%
  dplyr::select(-c(PrimarySatellite, SecondarySatellite))

# append new conjunctions to previous
all_conjs_expanded = rbind(firstSet, secondSet)
saveRDS(all_conjs_expanded, "RDSfiles/all_conjs_expanded") # save to RDS file

```

## Miss distance vs cumulative count plot
```{r miss_dist_cum_count_plot}
# axis ticks for log scale
ticks <- 1:10
ooms <- 10^(0:3) # define orders of magnitudes
breaks <- as.vector(ticks %o% ooms)

show.labels <- c(T, F, F, F, T, F, T, F)
labels <- as.character(breaks * show.labels)
labels <- gsub("^0$", "", labels)

all_conjs %>%
  group_by(clusterLab) %>%
  arrange(Range) %>%
  mutate(rowid = 1, cumnum = cumsum(rowid)) %>% 
  ggplot(aes(x=Range, y = cumnum, color=clusterLab)) + 
  geom_line() +
  theme_light() +
  scale_x_log10(labels = labels, breaks = breaks) +
  scale_y_log10(labels = labels, breaks = breaks) +
  labs(x="Miss Distance (km)", y="Cumulative Number of Encounters", color="Cluster",
       title="Cumulative Number of Encounters by Cluster", subtitle = paste0("Encounters from 20OCT2019-", today))+
  scale_color_brewer(palette="Set1")
```

## Percent of Encounters by Country

```{r perc_encounters_country, fig.width=7.5}
# read in country codes
country_codes = read_csv("./misc_files/country_codes.csv", 
                         col_names = c("country", "Country"), col_types = "cc", skip = 1) %>%
  mutate(Country = str_to_title(Country),
         Country = if_else(str_length(Country) > 20, country, Country))

# plot percent of encounters by country
p = all_conjs %>%
  mutate(noradId = as.numeric(gsub("--.*$", "", PrimarySatellite ))) %>%
  left_join(dplyr::select(mcma_objs, c(noradId, country)), by="noradId") %>%
  mutate(country = if_else(country == "CHBZ", "PRC", country)) %>%
  group_by(clusterLab, country) %>% 
  summarise(numEncounters = n()) %>%
  left_join(country_codes, by="country") %>% 
  group_by(clusterLab) %>%
  mutate(encountersPerClust = sum(numEncounters), 
         p = numEncounters / encountersPerClust * 100) %>%
  group_by(clusterLab) %>%
  mutate(country_new = if_else(p < 2, "Other", Country)) %>% 
  mutate(p = if_else(country_new=="Other", mean(p[country_new=="Other"]), p))

colourCount = length(unique(p$country_new))
getPalette = colorRampPalette(brewer.pal(9, "Set1"))

ggplot() + 
  geom_bar(data = p, aes(x=clusterLab, y=p/100, group=country_new, fill=country_new), stat="identity")+
  geom_text(data = unique(dplyr::select(p, c(clusterLab, encountersPerClust))), position = position_stack(vjust=1.05), 
            aes(x=clusterLab, y=1, label=encountersPerClust))+
  theme_minimal() +
  scale_y_continuous(labels = scales::percent) + 
  scale_fill_manual(values = getPalette(colourCount))+
  labs(x="Cluster",y="", title = "Percent of Encounters by Country", fill="Country",
       subtitle="Number of encounters shown above each bar", caption=paste0("Encounters from 20OCT2019-", today))
```


## Worst offender analysis-- Method 1

Using the below algorithm (with all weights equal to 1 for now-- will experiment with later): ![SMC derelicts graphic](misc_files/SMCderelicts.png)

```{r top50}
top50 = mcma_objs %>% 
  arrange(desc(totalRisk)) %>%
  head(50)


top50 = all_conjs_expanded %>%
  filter(noradId %in% top50$noradId) %>%
  group_by(noradId) %>%
  summarise(`num conjs` = n(),
            `closest approach (m)` = min(Range)*1000,
            `avg num op sats` = mean(numOpSats),
            `med num op sats` = median(numOpSats),
            `avg persist of conj alt` = mean(persistence),
            `med persist of conj alt` = median(persistence),
            `num encounters <1 km` = length(Range[Range<1])) %>% 
  right_join(top50, by="noradId")
```

Current list of top 50 objects with highest risk:
```{r view_top_50}
cols_ordered = c("noradId", "cluster_new", "totalRisk", "num conjs", "closest approach (m)", "avg num op sats","med num op sats", "avg persist of conj alt", "med persist of conj alt", "num encounters <1 km", "apogee", "perigee","launch","inclination","mass","type","country")

top50 %>% dplyr::select(cols_ordered) %>%
  rename(cluster=cluster_new,
         `total risk`=totalRisk) %>%
  DT::datatable(rownames=F, extensions = c('Buttons', 'Responsive'),
                options = list( lengthMenu = c(10, 20, 30, 40, 50),
                                buttons = list(list(extend='excel',filename="worst_offenders_1", text="Download data", title=paste0("Worst offenders (method 1) as of ", today))),
                                dom = 'Bfrtip'#,
                                )) %>%
  formatRound(columns = c("closest approach (m)","avg num op sats","med num op sats",
  "avg persist of conj alt","med persist of conj alt", "total risk"), digits=1) %>%
  formatDate(columns = "launch", method = "toDateString", params = list('en-US', list(year = 'numeric', month = 'numeric', day='numeric')
      ))

```

- `num conjs` is the total number of conjunctions (<5km) the object has been in (since 20OCT2019)
- `avg num op sats` is the average number of operational satellites within 100 km above or below each conjunction
- `med num op sats` is the same as `avg num op sats` but median instead of mean
- `avg persist of conj alt` is the average persistence (in years) of debris produced at the altitude of encounters (a rough approximation based on altitude)
- `med persist of conj alt` is the same as `avg persist of conj alt` but median instead of mean
- `num encounters <1 km` is the number of encounters below 1 km miss distance

### Risk among all objects
Among all objects, percent of risk accounted for by each cluster

```{r risk_plot}
mcma_objs %>% group_by(cluster_new) %>%
  summarise(p = n()) %>% 
  mutate(p = p/nrow(mcma_objs)*100,
         cluster_new = factor(cluster_new,
                             levels = c("cc615", "cc775", "cc850", "cc975", "cc1200", "cc1500", "cleo","CHIGH"),
                             ordered = T))%>%
  ggplot(aes(x=1, y=p, fill=cluster_new)) +
  geom_col(width = .5)+
  geom_text(position = position_stack(vjust=.5), 
            aes(label=paste0(round(p), "%")))+
  theme_minimal()+
  theme(axis.title.x = element_blank(), axis.text = element_blank(), 
        panel.grid.major = element_blank(), panel.grid.minor = element_blank())+
  labs(fill="Cluster", y="Risk")
```

## Worst offender analysis-- Method 2 (Darren's method)
